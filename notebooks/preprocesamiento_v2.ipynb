{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18dec39",
   "metadata": {},
   "source": [
    "# Fase 2: Preprocesamiento y Selección de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "2308d4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import joblib\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "7b5eb943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset crudo: 1185 días × 295 columnas\n",
      "Rango: 2020-07-01 00:00:00 -> 2023-10-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# carga de datos\n",
    "df_raw = pd.read_csv('../data/df_raw.csv', index_col=0, parse_dates=True)\n",
    "target_col = 'Consolidado EE_Frio (Kw)'\n",
    "\n",
    "print(f\"Dataset crudo: {df_raw.shape[0]} días × {df_raw.shape[1]} columnas\")\n",
    "print(f\"Rango: {df_raw.index.min()} -> {df_raw.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b9f89",
   "metadata": {},
   "source": [
    "## 2.1. Limpieza de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a9805e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tratamiento de errores \n",
    "\n",
    "# reemplazar errores comunes de Excel\n",
    "errors = ['#VALUE!', '#DIV/0!', '#REF!', '#N/A', '#NAME?', '#NUM!', '#NULL!']\n",
    "df = df_raw.replace(errors, np.nan, regex=True)\n",
    "\n",
    "# Convertir a numérico \n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "c39ef9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers >40,000 kW eliminados: 24\n",
      "Dataset tras limpieza: (1161, 295)\n"
     ]
    }
   ],
   "source": [
    "# eliminar outliers extremos\n",
    "sane_threshold = 40000  # kW\n",
    "outliers_before = len(df)\n",
    "\n",
    "df = df[df[target_col] < sane_threshold].copy()\n",
    "\n",
    "print(f\"Outliers >{sane_threshold:,} kW eliminados: {outliers_before - len(df)}\")\n",
    "print(f\"Dataset tras limpieza: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "58b81539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas con >80% missing: 0\n",
      "Columnas Unnamed: 1\n",
      "Total eliminadas: 1\n",
      "Dataset tras limpieza: (1161, 294)\n"
     ]
    }
   ],
   "source": [
    "# eliminar columnas con >80% missing o 'Unnamed'\n",
    "missing_pct = df.isna().mean()\n",
    "high_missing = missing_pct[missing_pct > 0.8].index\n",
    "unnamed_cols = df.columns[df.columns.str.contains('Unnamed', case=False)]\n",
    "\n",
    "cols_to_drop = high_missing.union(unnamed_cols)\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "\n",
    "print(f\"Columnas con >80% missing: {len(high_missing)}\")\n",
    "print(f\"Columnas Unnamed: {len(unnamed_cols)}\")\n",
    "print(f\"Total eliminadas: {len(cols_to_drop)}\")\n",
    "print(f\"Dataset tras limpieza: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "57342e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes antes de imputación: 10021\n",
      "Valores faltantes tras imputación: 0\n"
     ]
    }
   ],
   "source": [
    "# imputación de valores faltantes\n",
    "missing_before = df.isna().sum().sum()\n",
    "print(f\"Valores faltantes antes de imputación: {missing_before}\")\n",
    "df = df.sort_index()\n",
    "\n",
    "# 1. Forward fill (serie temporal)\n",
    "df = df.ffill()\n",
    "\n",
    "# 2. Backward fill (para huecos al inicio)\n",
    "df = df.bfill()\n",
    "\n",
    "# 3. Mediana como fallback\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "missing_after = df.isna().sum().sum()\n",
    "print(f\"Valores faltantes tras imputación: {missing_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72a9f5",
   "metadata": {},
   "source": [
    "## 2.2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ca368e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables temporales creadas: dia de la semana, mes, es fin de semana\n",
      "Shape final: (1161, 297)\n"
     ]
    }
   ],
   "source": [
    "# creación de variables temporales\n",
    "\n",
    "# extraer componentes de fecha\n",
    "date_features = pd.DataFrame({\n",
    "    'day_of_week': df.index.dayofweek,    # 0 = lunes\n",
    "    'month': df.index.month,\n",
    "    'is_weekend': df.index.dayofweek.isin([5, 6]).astype(int)\n",
    "}, index=df.index)\n",
    "\n",
    "df = pd.concat([df, date_features], axis=1)\n",
    "\n",
    "print(\"Variables temporales creadas: dia de la semana, mes, es fin de semana\")\n",
    "print(f\"Shape final: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3fc1b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear lags y rolling statistics\n",
    "df['lag_1'] = df[target_col].shift(1)\n",
    "df['lag_7'] = df[target_col].shift(7)\n",
    "df['rolling_7'] = df[target_col].rolling(window=7).mean()\n",
    "df['rolling_30'] = df[target_col].rolling(window=30).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "3b9c5070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producción total (Hl): 13 columnas sumadas\n"
     ]
    }
   ],
   "source": [
    "# crear variable de producción total (Hl) y eficiencia por Hl\n",
    "hl_cols = [col for col in df.columns if 'Hl ' in col and 'Most' not in col]\n",
    "df['hl_total'] = df[hl_cols].sum(axis=1)\n",
    "\n",
    "df['ee_frio_por_hl'] = df[target_col] / (df['hl_total'] + 1e-6)  # evitar división por 0\n",
    "\n",
    "print(f\"Producción total (Hl): {len(hl_cols)} columnas sumadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7d2ca0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta detectada: 22 columnas\n",
      "  → ['Consolidado KPI_EE Planta / Hl', 'Consolidado KPI_EE Elaboracion / Hl', 'Consolidado KPI_EE Bodega / Hl']...\n",
      "  → 'eficiencia_frio' = meta_promedio / consumo_real\n"
     ]
    }
   ],
   "source": [
    "# crear variable de eficiencia energética (real vs meta)\n",
    "\n",
    "# búsqueda flexible de columnas \"Meta\" relacionadas con Frío o KPI\n",
    "meta_candidates = [\n",
    "    col for col in df.columns \n",
    "    if any(keyword in col.lower() for keyword in ['meta', 'kpi', 'objetivo'])\n",
    "    and any(area in col.lower() for area in ['frio', 'ee', 'planta'])\n",
    "]\n",
    "\n",
    "if meta_candidates:\n",
    "    # Tomamos promedio de todas las metas relevantes\n",
    "    df['meta_frio_promedio'] = df[meta_candidates].mean(axis=1, skipna=True)\n",
    "    df['eficiencia_frio'] = df['meta_frio_promedio'] / (df[target_col] + 1e-6)\n",
    "    print(f\"Meta detectada: {len(meta_candidates)} columnas\")\n",
    "    print(f\"  → {meta_candidates[:3]}...\")\n",
    "    print(\"  → 'eficiencia_frio' = meta_promedio / consumo_real\")\n",
    "else:\n",
    "    # Si no hay meta, usar ratio por Hl \n",
    "    hl_cols = [c for c in df.columns if 'Hl ' in c]\n",
    "    if hl_cols:\n",
    "        df['hl_total'] = df[hl_cols].sum(axis=1)\n",
    "        df['eficiencia_frio'] = df[target_col] / (df['hl_total'] + 1e-6)\n",
    "        print(\"No hay 'Meta', entonces usando kW/Hl como proxy de eficiencia\")\n",
    "    else:\n",
    "        df['eficiencia_frio'] = 1.0  # fallback\n",
    "        print(\"No hay datos para eficiencia y por lo tanto, valor 1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "acfdbcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interacciones creadas: 2 -> ['resto × mycom7', 'agua_cond × frio']\n"
     ]
    }
   ],
   "source": [
    "# creación de interacciones\n",
    "interacciones = []\n",
    "\n",
    "# 1. Resto Serv × Mycom 7\n",
    "col_resto = 'Consolidado EE_Resto Serv (Kw)'\n",
    "col_mycom7 = next((c for c in df.columns if 'Mycom 7' in c), None)\n",
    "\n",
    "if col_resto in df.columns and col_mycom7:\n",
    "    df['inter_resto_x_mycom7'] = df[col_resto] * df[col_mycom7]\n",
    "    interacciones.append('resto × mycom7')\n",
    "else:\n",
    "    print(f\"Advertencia: No se encontró '{col_mycom7}' para interacción\")\n",
    "\n",
    "# 2. Agua Condensada × Frio\n",
    "col_agua = next((c for c in df.columns if 'agua' in c.lower() and 'cond' in c.lower()), None)\n",
    "\n",
    "if col_agua and target_col in df.columns:\n",
    "    df['inter_agua_cond_x_frio'] = df[col_agua] * df[target_col]\n",
    "    interacciones.append('agua_cond × frio')\n",
    "else:\n",
    "    print(f\"Advertencia: No se encontró columna de agua condensada\")\n",
    "\n",
    "# 3. Fallback: interacción con driver más correlacionado\n",
    "if len(interacciones) == 0:\n",
    "    top_corr = 'Consolidado EE_Sala Maq (Kw)'\n",
    "    if top_corr in df.columns:\n",
    "        df['inter_fallback'] = df[top_corr] * df[target_col]\n",
    "        interacciones.append('sala_maq × frio')\n",
    "        print(\"Usando fallback: Sala Maq × Frio\")\n",
    "\n",
    "print(f\"Interacciones creadas: {len(interacciones)} -> {interacciones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d433741",
   "metadata": {},
   "source": [
    "## 2.3 Selección de Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82e086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas antes de dropna(): 1161\n",
      "Filas después de dropna(): 1131\n",
      "0 NaN → listo para modelado\n",
      "Train: 840 días\n",
      "Test:  291 días\n",
      "Features shape (sin Frio(t)): (840, 306)\n"
     ]
    }
   ],
   "source": [
    "# creación de target y split temporal\n",
    "\n",
    "target_col = 'Consolidado EE_Frio (Kw)'\n",
    "\n",
    "# 1. Crear target = Frio(t+1)\n",
    "df['target'] = df[target_col].shift(-1)\n",
    "\n",
    "# 2. Eliminar filas con NaN (por shift, rolling, etc.)\n",
    "print(f\"Filas antes de dropna(): {len(df)}\")\n",
    "df = df.dropna()\n",
    "print(f\"Filas después de dropna(): {len(df)}\")\n",
    "\n",
    "# 3. Verificación\n",
    "assert df.isna().sum().sum() == 0, \"Toda hay NaN\"\n",
    "print(\"0 NaN → listo para modelado\")\n",
    "\n",
    "# 4. Split temporal\n",
    "train = df[df.index.year <= 2022].copy()\n",
    "test = df[df.index.year == 2023].copy()\n",
    "\n",
    "print(f\"Train: {len(train)} días\")\n",
    "print(f\"Test:  {len(test)} días\")\n",
    "\n",
    "# 5. Separar X/y y excluir Frio(t)\n",
    "X_train = train.drop(columns=['target', target_col])  # ¡EXCLUIR FRIO(t)!\n",
    "y_train = train['target']\n",
    "\n",
    "X_test = test.drop(columns=['target', target_col])\n",
    "y_test = test['target']\n",
    "\n",
    "print(f\"Features shape (sin Frio(t)): {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5b08e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 15 (Random Forest):\n",
      "rolling_7                                        0.5518\n",
      "Consolidado EE_Sala Maq (Kw)                     0.1269\n",
      "Consolidado EE_Servicios (Kw)                    0.0374\n",
      "Servicios_total                                  0.0276\n",
      "inter_agua_cond_x_frio                           0.0116\n",
      "rolling_30                                       0.0069\n",
      "Consolidado EE_Planta (Kw)                       0.0055\n",
      "Totalizadores Energia_KW Gral Planta             0.0042\n",
      "Totalizadores Energia_KW Laboratorio             0.0041\n",
      "Totalizadores Energia_KW Linea 3                 0.0038\n",
      "Totalizadores Energia_KW Cond 5. 6 y 9           0.0035\n",
      "eficiencia_frio                                  0.0033\n",
      "Consolidado EE_Restos Planta (Kw)                0.0030\n",
      "Totalizadores Energia_KW Obrador Contratistas    0.0029\n",
      "Consolidado EE_KW Gral Planta                    0.0028\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 2.3.2 - Random Forest para importancia de variables (sin Frio(t))\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "top_15_rf = importances.nlargest(15)\n",
    "\n",
    "print(\"TOP 15 (Random Forest):\")\n",
    "print(top_15_rf.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9344ba86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFE seleccionó 15 variables:\n",
      "   1. Consolidado KPI_EE Planta / Hl\n",
      "   2. Consolidado KPI_EE Elaboracion / Hl\n",
      "   3. Consolidado KPI_EE Bodega / Hl\n",
      "   4. Consolidado KPI_EE Envasado / Hl\n",
      "   5. Consolidado KPI_EE Servicios / Hl\n",
      "   6. Consolidado KPI_EE Frio / Hl\n",
      "   7. Consolidado KPI_EE Aire / Hl\n",
      "   8. Consolidado KPI_EE Caldera / Hl\n",
      "   9. Consolidado KPI_EE Agua / Hl\n",
      "  10. Consolidado KPI_EE Resto Serv / Hl\n",
      "  11. Consolidado KPI_EE Resto Planta / Hl\n",
      "  12. Consolidado KPI_Agua Planta / Hl\n",
      "  13. Consolidado KPI_Aire Planta / Hl\n",
      "  14. Consolidado KPI_CO 2 linea 3 / Hl\n",
      "  15. Consolidado KPI_CO 2 Linea 4 / Hl\n"
     ]
    }
   ],
   "source": [
    "# 2.3.3 - RFE (sin Frio(t))\n",
    "\n",
    "model = LinearRegression()\n",
    "rfe = RFE(estimator=model, n_features_to_select=15)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "selected_rfe = X_train.columns[rfe.support_].tolist()\n",
    "\n",
    "print(f\"RFE seleccionó {len(selected_rfe)} variables:\")\n",
    "for i, col in enumerate(selected_rfe, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9e21fcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables finales: 30\n",
      "   1. Consolidado EE_KW Gral Planta\n",
      "   2. Consolidado EE_Planta (Kw)\n",
      "   3. Consolidado EE_Restos Planta (Kw)\n",
      "   4. Consolidado EE_Sala Maq (Kw)\n",
      "   5. Consolidado EE_Servicios (Kw)\n",
      "   6. Consolidado KPI_Agua Planta / Hl\n",
      "   7. Consolidado KPI_Aire Planta / Hl\n",
      "   8. Consolidado KPI_CO 2 Linea 4 / Hl\n",
      "   9. Consolidado KPI_CO 2 linea 3 / Hl\n",
      "  10. Consolidado KPI_EE Agua / Hl\n",
      "  11. Consolidado KPI_EE Aire / Hl\n",
      "  12. Consolidado KPI_EE Bodega / Hl\n",
      "  13. Consolidado KPI_EE Caldera / Hl\n",
      "  14. Consolidado KPI_EE Elaboracion / Hl\n",
      "  15. Consolidado KPI_EE Envasado / Hl\n",
      "  16. Consolidado KPI_EE Frio / Hl\n",
      "  17. Consolidado KPI_EE Planta / Hl\n",
      "  18. Consolidado KPI_EE Resto Planta / Hl\n",
      "  19. Consolidado KPI_EE Resto Serv / Hl\n",
      "  20. Consolidado KPI_EE Servicios / Hl\n",
      "  21. Servicios_total\n",
      "  22. Totalizadores Energia_KW Cond 5. 6 y 9\n",
      "  23. Totalizadores Energia_KW Gral Planta\n",
      "  24. Totalizadores Energia_KW Laboratorio\n",
      "  25. Totalizadores Energia_KW Linea 3\n",
      "  26. Totalizadores Energia_KW Obrador Contratistas\n",
      "  27. eficiencia_frio\n",
      "  28. inter_agua_cond_x_frio\n",
      "  29. rolling_30\n",
      "  30. rolling_7\n",
      "Dataset final shape: (1131, 31)\n"
     ]
    }
   ],
   "source": [
    "# 2.3.4 - Conjunto final (unión RF + RFE)\n",
    "\n",
    "final_features = list(set(top_15_rf.index) | set(selected_rfe))\n",
    "\n",
    "# eliminar target_col si está presente \n",
    "if target_col in final_features:\n",
    "    final_features.remove(target_col)\n",
    "\n",
    "print(f\"Variables finales: {len(final_features)}\")\n",
    "for i, col in enumerate(sorted(final_features), 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "\n",
    "# Dataset final (solo features + target(t+1))\n",
    "df_final = df[final_features + ['target']].copy()\n",
    "print(f\"Dataset final shape: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4403b5",
   "metadata": {},
   "source": [
    "## 2.4. Preparación y Versionado de Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "926e41a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled: (840, 30)\n",
      "X_test_scaled:  (291, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../models/scaler.pkl']"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.4.1 - Escalado (RobustScaler)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[final_features])\n",
    "X_test_scaled = scaler.transform(X_test[final_features])\n",
    "\n",
    "print(f\"X_train_scaled: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled:  {X_test_scaled.shape}\")\n",
    "\n",
    "# Guardar scaler\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "60ec2386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_final.csv guardado (sin escalar)\n",
      "X_train, X_test, y_train, y_test guardados (escalados)\n"
     ]
    }
   ],
   "source": [
    "# 2.5.1 - Guardar datasets procesados\n",
    "\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# 1. dataset_final.csv (sin escalar)\n",
    "dataset_final = df_final.copy()  # ya tiene final_features + 'target'\n",
    "dataset_final.to_csv('../data/processed/dataset_final.csv')\n",
    "print(\"dataset_final.csv guardado (sin escalar)\")\n",
    "\n",
    "# 2. X_train, X_test, y_train, y_test (escalados)\n",
    "pd.DataFrame(X_train_scaled, columns=final_features).to_csv('../data/processed/X_train.csv', index=False)\n",
    "pd.DataFrame(X_test_scaled, columns=final_features).to_csv('../data/processed/X_test.csv', index=False)\n",
    "pd.Series(y_train).to_csv('../data/processed/y_train.csv', header=['target'], index=False)\n",
    "pd.Series(y_test).to_csv('../data/processed/y_test.csv', header=['target'], index=False)\n",
    "\n",
    "print(\"X_train, X_test, y_train, y_test guardados (escalados)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "447528dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linaje y checksum guardados\n"
     ]
    }
   ],
   "source": [
    "# 3. linaje y checksum\n",
    "\n",
    "# Linaje\n",
    "lineage = {\n",
    "    \"fecha\": datetime.now().isoformat(),\n",
    "    \"script\": \"preprocesamiento.ipynb\",\n",
    "    \"rama\": \"feature/preprocessing\",\n",
    "    \"outliers\": \">40,000 kW eliminados\",\n",
    "    \"imputacion\": \"ffill + bfill + mediana\",\n",
    "    \"feature_engineering\": [\"lag_7\", \"rolling_7\", \"rolling_30\", \"eficiencia_frio\", \"interacciones\"],\n",
    "    \"seleccion\": \"RF + RFE → 29 variables\",\n",
    "    \"split\": \"Train ≤2022, Test 2023\",\n",
    "    \"shapes\": {\n",
    "        \"X_train\": X_train_scaled.shape,\n",
    "        \"X_test\": X_test_scaled.shape\n",
    "    },\n",
    "    \"archivos_guardados\": [\n",
    "    \"dataset_final.csv\",\n",
    "    \"X_train.csv\", \"X_test.csv\",\n",
    "    \"y_train.csv\", \"y_test.csv\",\n",
    "    \"scaler.pkl\"\n",
    "]\n",
    "}\n",
    "\n",
    "with open('../data/processed/data_lineage.json', 'w') as f:\n",
    "    json.dump(lineage, f, indent=2)\n",
    "\n",
    "# Checksum\n",
    "def md5(obj):\n",
    "    return hashlib.md5(str(obj).encode()).hexdigest()\n",
    "\n",
    "checksums = {\n",
    "    \"X_train\": md5(X_train_scaled.tolist()),\n",
    "    \"X_test\": md5(X_test_scaled.tolist()),\n",
    "    \"y_train\": md5(y_train.tolist()),\n",
    "    \"y_test\": md5(y_test.tolist())\n",
    "}\n",
    "\n",
    "with open('../data/checksums.json', 'w') as f:\n",
    "    json.dump(checksums, f, indent=2)\n",
    "\n",
    "print(\"Linaje y checksum guardados\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cervecera_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
