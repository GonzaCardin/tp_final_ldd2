{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4711ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# librerías\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import reduce # lo usamos para los merges múltiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fd5f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos encontrados: 3\n",
      "['../data\\\\Totalizadores Planta de Cerveza - 2022_2023.xlsx', '../data\\\\Totalizadores Planta de Cerveza 2021_2022.xlsx', '../data\\\\Totalizadores Planta de Cerveza 2023_2024.xlsx']\n"
     ]
    }
   ],
   "source": [
    "# verificamos la ruta de los datos y si se encuentran ahi\n",
    "data_path = '../data'\n",
    "excel_files = glob.glob(os.path.join(data_path, '*.xlsx'))\n",
    "print(f'Archivos encontrados: {len(excel_files)}')\n",
    "print(excel_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9418e246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hojas a cargar: ['Consolidado EE', 'Consolidado Produccion', 'Consolidado GasVapor', 'Totalizadores Energia']\n",
      "Cargando archivo: ../data\\Totalizadores Planta de Cerveza - 2022_2023.xlsx\n",
      " Hoja Consolidado EE leída correctamente.\n",
      " Hoja Consolidado Produccion leída correctamente.\n",
      " Hoja Consolidado GasVapor leída correctamente.\n",
      " Hoja Totalizadores Energia leída correctamente.\n",
      "Cargando archivo: ../data\\Totalizadores Planta de Cerveza 2021_2022.xlsx\n",
      " Hoja Consolidado EE leída correctamente.\n",
      " Hoja Consolidado Produccion leída correctamente.\n",
      " Hoja Consolidado GasVapor leída correctamente.\n",
      " Hoja Totalizadores Energia leída correctamente.\n",
      "Cargando archivo: ../data\\Totalizadores Planta de Cerveza 2023_2024.xlsx\n",
      " Hoja Consolidado EE leída correctamente.\n",
      " Hoja Consolidado Produccion leída correctamente.\n",
      " Hoja Consolidado GasVapor leída correctamente.\n",
      " Hoja Totalizadores Energia leída correctamente.\n"
     ]
    }
   ],
   "source": [
    "# carga y lectura de los datos\n",
    "\n",
    "# primero definimos las hojas que vamos a cargar\n",
    "sheets_to_load = [\n",
    "    'Consolidado EE',\n",
    "    'Consolidado Produccion',\n",
    "    'Consolidado GasVapor',\n",
    "    'Totalizadores Energia'\n",
    "]\n",
    "\n",
    "print(f'Hojas a cargar: {sheets_to_load}')\n",
    "\n",
    "# ahora vamos a leer y almacenar los datos en un diccionario\n",
    "all_sheets_data = {sheet_name: [] for sheet_name in sheets_to_load}\n",
    "\n",
    "# realizamos un pequño control para ver si los archivos existen\n",
    "if 'excel_files' not in locals() or not excel_files:\n",
    "    raise FileNotFoundError(\"No se encontraron archivos Excel en la ruta especificada.\")\n",
    "for file in excel_files:\n",
    "    print(f'Cargando archivo: {file}')\n",
    "    try:\n",
    "        xls = pd.ExcelFile(file)\n",
    "        for sheet_name in sheets_to_load:\n",
    "            if sheet_name in xls.sheet_names:\n",
    "                try:\n",
    "                    df_sheet = xls.parse(sheet_name=sheet_name) # leemos la hoja\n",
    "                    df_sheet.columns = df_sheet.columns.str.strip() # limpiamos los nombres de las columnas\n",
    "                    all_sheets_data[sheet_name].append(df_sheet) # almacenamos el dataframe\n",
    "                    print(f' Hoja {sheet_name} leída correctamente.')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al procesar la hoja {sheet_name} en el archivo {file}: {e}\")\n",
    "            else:\n",
    "                print(f\" Hoja {sheet_name} no encontrada en el archivo {file}.\")\n",
    "    except Exception as e_file:\n",
    "        print(f\"Error al abrir el archivo {file}: {e_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d7846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoja Consolidado EE concatenada: 43515 filas, 24 columnas.\n",
      "Hoja Consolidado Produccion concatenada: 43034 filas, 19 columnas.\n",
      "Hoja Consolidado GasVapor concatenada: 43520 filas, 21 columnas.\n",
      "Hoja Totalizadores Energia concatenada: 42900 filas, 60 columnas.\n",
      "Dataframe base Consolidado EE preparado para merge: 43515 filas, 24 columnas.\n",
      "Merge con hoja Consolidado Produccion realizado: 43515 filas, 41 columnas.\n",
      "Merge con hoja Consolidado GasVapor realizado: 43515 filas, 60 columnas.\n",
      "Merge con hoja Totalizadores Energia realizado: 43515 filas, 118 columnas.\n"
     ]
    }
   ],
   "source": [
    "# combinamos los dataframes de cada hoja\n",
    "concatenated_data = {}\n",
    "for sheet_name, df_list in all_sheets_data.items():\n",
    "    if df_list:  # verificamos que la lista no esté vacía\n",
    "        concatenated_data[sheet_name] = pd.concat(df_list, ignore_index=True)\n",
    "        print(f'Hoja {sheet_name} concatenada: {concatenated_data[sheet_name].shape[0]} filas, {concatenated_data[sheet_name].shape[1]} columnas.')\n",
    "    else:\n",
    "        print(f'No se encontraron datos para la hoja {sheet_name}.')\n",
    "\n",
    "# verificamos que los dataframes concatenados existan y usamos 'Consolidado EE' como base para el merge\n",
    "base_sheet = sheets_to_load[0]\n",
    "if base_sheet in concatenated_data:\n",
    "    # preparamos antes los nombres de las columnas para dataframe base\n",
    "    concatenated_data[base_sheet].columns = concatenated_data[base_sheet].columns.str.strip() # limpiamos los nombres de las columnas\n",
    "    df_unified_raw = concatenated_data[base_sheet].copy() # empezamos con la hoja del target\n",
    "    print(f'Dataframe base {base_sheet} preparado para merge: {df_unified_raw.shape[0]} filas, {df_unified_raw.shape[1]} columnas.')\n",
    "    \n",
    "    # ahora iteramos sobre las otras hojas para hacer los merges\n",
    "    for sheet_name in sheets_to_load[1:]: # saltamos la primera hoja que ya usamos\n",
    "        if sheet_name in concatenated_data:\n",
    "            try:\n",
    "                # preparamos los nombres de las columnas para el dataframe a mergear\n",
    "                concatenated_data[sheet_name].columns = concatenated_data[sheet_name].columns.str.strip() # limpiamos los nombres de las columnas\n",
    "                df_to_merge = concatenated_data[sheet_name]\n",
    "\n",
    "                # para que estén ordenados por fecha, nos aseguramos que 'DIA' y 'HORA' existan y sean del tipo datetime\n",
    "                if 'DIA' in df_to_merge.columns and 'HORA' in df_to_merge.columns:\n",
    "                    df_to_merge['DIA'] = pd.to_datetime(df_to_merge['DIA'], errors='coerce')\n",
    "                    # no convertimos 'HORA' y nos aseguramos que 'DIA' en el dataframe base también sea datetime\n",
    "                    if 'DIA' in df_unified_raw.columns:\n",
    "                        df_unified_raw['DIA'] = pd.to_datetime(df_unified_raw['DIA'], errors='coerce')\n",
    "                    else:\n",
    "                        print(f\"La columna 'DIA' no existe en el dataframe base.\")\n",
    "                    \n",
    "                    # seleccionamos la columnas unicas de estas hojas, excluyendo 'DIA' y 'HORA'\n",
    "                    unique_columns = [col for col in df_to_merge.columns if col not in ['DIA', 'HORA']]\n",
    "                    merge_columns = ['DIA', 'HORA'] + unique_columns\n",
    "                    \n",
    "                    # realizamos el merge\n",
    "                    df_unified_raw = pd.merge(\n",
    "                        df_unified_raw,\n",
    "                        df_to_merge[merge_columns].drop_duplicates(subset=['DIA', 'HORA']),# evitamos duplicados en las claves de merge\n",
    "                        on=['DIA', 'HORA'],\n",
    "                        how='left'  # usamos left join para mantener todas las filas del dataframe base\n",
    "                    )\n",
    "                    print(f'Merge con hoja {sheet_name} realizado: {df_unified_raw.shape[0]} filas, {df_unified_raw.shape[1]} columnas.')\n",
    "                else:\n",
    "                    print(f\"Las columnas 'DIA' y/o 'HORA' no existen en la hoja {sheet_name}.\")\n",
    "            except Exception as e_merge:\n",
    "                print(f\"Error al hacer merge con la hoja {sheet_name}: {e_merge}\")\n",
    "        else:\n",
    "            print(f'No se encontraron datos para la hoja {sheet_name}, no se realizó merge.')\n",
    "    \n",
    "    \"\"\" mostramos el resultado final del dataframe unificado y lo guardamos\n",
    "    if 'df_unified_raw' in locals() and not df_unified_raw.empty:\n",
    "        print(f'Resultado final del dataframe unificado: {df_unified_raw.shape[0]} filas, {df_unified_raw.shape[1]} columnas.')\n",
    "        print('\\nPrimeras filas del dataframe unificado:')\n",
    "        print(df_unified_raw.head())\n",
    "        # guardamos el dataframe unificado en un archivo CSV\n",
    "        output_file = os.path.join(data_path, 'df_unified_raw.csv')\n",
    "        try:\n",
    "            df_unified_raw.to_csv(output_file, index=False, decimal='.')\n",
    "            print(f'Dataframe unificado guardado en {output_file}')\n",
    "        except Exception as e_save:\n",
    "            print(f\"Error al guardar el dataframe unificado: {e_save}\")\n",
    "    else:\n",
    "        print('\\n No se pudo crear el dataframe unificado.')\n",
    "        df_unified_raw = pd.DataFrame()  # creamos un dataframe vacío para evitar errores posteriores\n",
    "    \"\"\"\n",
    "else:\n",
    "    print(f'No se encontró la hoja base {base_sheet} en los datos concatenados.')\n",
    "    df_unified_raw = pd.DataFrame()  # creamos un dataframe vacío para evitar errores posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f870f398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Información del DataFrame unificado TOTAL:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43515 entries, 0 to 43514\n",
      "Data columns (total 118 columns):\n",
      " #    Column                    Non-Null Count  Dtype         \n",
      "---   ------                    --------------  -----         \n",
      " 0    DIA                       42900 non-null  datetime64[ns]\n",
      " 1    HORA                      42900 non-null  object        \n",
      " 2    Planta (Kw)               42902 non-null  float64       \n",
      " 3    Elaboracion (Kw)          42902 non-null  float64       \n",
      " 4    Bodega (Kw)               42902 non-null  float64       \n",
      " 5    Cocina (Kw)               42902 non-null  float64       \n",
      " 6    Envasado (Kw)             42902 non-null  float64       \n",
      " 7    Linea 2 (Kw)              42894 non-null  float64       \n",
      " 8    Linea 3 (Kw)              42902 non-null  float64       \n",
      " 9    Linea 4 (Kw)              42902 non-null  float64       \n",
      " 10   Servicios (Kw)            42902 non-null  float64       \n",
      " 11   Sala Maq (Kw)             42902 non-null  float64       \n",
      " 12   Aire (Kw)                 42902 non-null  float64       \n",
      " 13   Calderas (Kw)             42902 non-null  float64       \n",
      " 14   Efluentes (Kw)            42902 non-null  float64       \n",
      " 15   Frio (Kw)                 42902 non-null  float64       \n",
      " 16   Pta Agua / Eflu (Kw)      42902 non-null  float64       \n",
      " 17   Prod Agua (Kw)            42902 non-null  float64       \n",
      " 18   Resto Serv (Kw)           42902 non-null  float64       \n",
      " 19   Restos Planta (Kw)        43515 non-null  float64       \n",
      " 20   KW Gral Planta_x          42898 non-null  float64       \n",
      " 21   KW CO2_x                  27325 non-null  float64       \n",
      " 22   Fecha/Hora_x              27461 non-null  object        \n",
      " 23   Kw de Frio                27327 non-null  float64       \n",
      " 24   Hl de Mosto               43506 non-null  float64       \n",
      " 25   Hl Cerveza Cocina         43506 non-null  float64       \n",
      " 26   Hl Producido Bodega       43506 non-null  float64       \n",
      " 27   Hl Cerveza Filtrada       43506 non-null  float64       \n",
      " 28   Hl Cerveza Envasada       43506 non-null  float64       \n",
      " 29   Hl Cerveza L2             42891 non-null  float64       \n",
      " 30   Hl Cerveza L3             42891 non-null  float64       \n",
      " 31   Hl Cerveza L4             42891 non-null  float64       \n",
      " 32   Hl Cerveza L5             42891 non-null  float64       \n",
      " 33   Cocimientos Diarios       36469 non-null  float64       \n",
      " 34   Fecha/Hora_y              34383 non-null  object        \n",
      " 35   Hl de Mosto Copia         34383 non-null  float64       \n",
      " 36   Unnamed: 14               0 non-null      float64       \n",
      " 37   Unnamed: 15               1069 non-null   float64       \n",
      " 38   Unnamed: 16               1069 non-null   float64       \n",
      " 39   Unnamed: 17               1069 non-null   float64       \n",
      " 40   Unnamed: 18               1069 non-null   float64       \n",
      " 41   Conversion Kg/Mj          43515 non-null  float64       \n",
      " 42   Gas Planta (Mj)           43515 non-null  float64       \n",
      " 43   Vapor Elaboracion (Kg)    43515 non-null  float64       \n",
      " 44   Vapor Cocina (Kg)         43515 non-null  float64       \n",
      " 45   Vapor Envasado (Kg)       43515 non-null  float64       \n",
      " 46   Vapor Servicio (Kg)       42900 non-null  float64       \n",
      " 47   ET Elaboracion (Mj)       43515 non-null  float64       \n",
      " 48   ET Envasado (Mj)          43515 non-null  float64       \n",
      " 49   ET Servicios (Mj)         43515 non-null  float64       \n",
      " 50   Tot_Vapor_L3_L4           42900 non-null  float64       \n",
      " 51   VAPOR DE LINEA 1 Y 2 KG   42899 non-null  float64       \n",
      " 52   VAPOR DE LINEA 4 KG       42900 non-null  float64       \n",
      " 53   Vapor_L5 (KG)             33774 non-null  float64       \n",
      " 54   Tot_Vapor_CIP_Bodega      42900 non-null  float64       \n",
      " 55   Vapor L3                  43515 non-null  float64       \n",
      " 56   Tot Vap Paste L3 / Hora   34387 non-null  float64       \n",
      " 57   Tot Vap Lav L3 / Hora     34387 non-null  float64       \n",
      " 58   Medicion Gas Planta (M3)  33774 non-null  float64       \n",
      " 59   Vapor _Vapor_L5 (KG)      9126 non-null   float64       \n",
      " 60   KW Gral Planta_y          42898 non-null  float64       \n",
      " 61   KW Trafo 4                42900 non-null  float64       \n",
      " 62   KW Trafo 5                42900 non-null  float64       \n",
      " 63   KW Trafo 8                42900 non-null  object        \n",
      " 64   KW Trafo 11               42900 non-null  float64       \n",
      " 65   KW Trafo 12               42900 non-null  float64       \n",
      " 66   KW Linea 2                42900 non-null  float64       \n",
      " 67   KW Mycom 1                42900 non-null  float64       \n",
      " 68   KW Mycom 2                42898 non-null  float64       \n",
      " 69   KW Mycom 3                42900 non-null  float64       \n",
      " 70   KW Mycom 4                42900 non-null  float64       \n",
      " 71   KW Mycom 5                42900 non-null  float64       \n",
      " 72   KW Mycom 6                42900 non-null  float64       \n",
      " 73   KW Mycom 7                42900 non-null  float64       \n",
      " 74   KW Trafo 9                42900 non-null  float64       \n",
      " 75   KW Trafo 10               42900 non-null  float64       \n",
      " 76   KW Cocina                 42900 non-null  float64       \n",
      " 77   Kw Molino                 42900 non-null  float64       \n",
      " 78   KW Linea 3 y 4            42900 non-null  float64       \n",
      " 79   KW Linea 3                42900 non-null  float64       \n",
      " 80   KW Linea 4                42900 non-null  float64       \n",
      " 81   KW Servicio L2            42900 non-null  float64       \n",
      " 82   KW Planta de Agua         42900 non-null  float64       \n",
      " 83   Kw llum/Serv L2           42899 non-null  float64       \n",
      " 84   Kw Casona                 42899 non-null  float64       \n",
      " 85   KW Laboratorio            42899 non-null  float64       \n",
      " 86   Kw Admininistracion       42899 non-null  float64       \n",
      " 87   KW Pta Agua/Log           42899 non-null  float64       \n",
      " 88   KW Bba Glicol Sala MAq    42899 non-null  float64       \n",
      " 89   KW Comp Kaeser            42899 non-null  float64       \n",
      " 90   KW Iluminacion L3         42899 non-null  float64       \n",
      " 91   KW Iluminacion L4         42899 non-null  float64       \n",
      " 92   KW Ilum Dep L3/L4         42899 non-null  float64       \n",
      " 93   KW Cond 5. 6 y 9          42899 non-null  float64       \n",
      " 94   KW Cond 7. 8 y 11         42899 non-null  float64       \n",
      " 95   KW Cond 11. 12 y 13       42899 non-null  float64       \n",
      " 96   KW Bba Glicol Bod         42899 non-null  float64       \n",
      " 97   KW CO2_y                  42899 non-null  float64       \n",
      " 98   KW Secador Kaeser         42899 non-null  float64       \n",
      " 99   KW Caldera 3              42899 non-null  float64       \n",
      " 100  KW Caldera 4              42899 non-null  float64       \n",
      " 101  KW Filtr Carbon           42899 non-null  float64       \n",
      " 102  KW Atlas 3                42899 non-null  float64       \n",
      " 103  KW Toma Agua              42899 non-null  float64       \n",
      " 104  KW Enfr Agua Cocina       42899 non-null  float64       \n",
      " 105  KW Enfluentes Coc         42899 non-null  float64       \n",
      " 106  KW Enfluente Efl          42899 non-null  float64       \n",
      " 107  KW Enfluentes Hidr        42899 non-null  float64       \n",
      " 108  KW Obrador Contratistas   42899 non-null  float64       \n",
      " 109  Kw Compresores Aire       42900 non-null  float64       \n",
      " 110  Kw Linea Barriles         42899 non-null  float64       \n",
      " 111  Unnamed: 53               1499 non-null   float64       \n",
      " 112  Unnamed: 54               1 non-null      float64       \n",
      " 113  Unnamed: 55               1 non-null      float64       \n",
      " 114  Unnamed: 56               1 non-null      float64       \n",
      " 115  Unnamed: 57               1 non-null      float64       \n",
      " 116  Unnamed: 58               1 non-null      float64       \n",
      " 117  Id                        5810 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(113), object(4)\n",
      "memory usage: 39.2+ MB\n"
     ]
    }
   ],
   "source": [
    "print('\\nInformación del DataFrame unificado TOTAL:')\n",
    "pd.options.display.max_info_columns = df_unified_raw.shape[1] + 1 # Mostrar todas las columnas\n",
    "df_unified_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4aa3328e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columnas a eliminar: ['Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Id', 'Fecha/Hora_x', 'Kw de Frio']\n",
      "Dataframe después de eliminar columnas redundantes: 43515 filas, 104 columnas.\n"
     ]
    }
   ],
   "source": [
    "# procedemos a realizar una limpieza básica del dataframe unificado\n",
    "\n",
    "# 1. identificamos y eliminamos columnas redundantes\n",
    "columns_unnamed = [col for col in df_unified_raw.columns if 'Unnamed' in col]\n",
    "cols_id = [col for col in df_unified_raw.columns if col.lower() == 'id']\n",
    "# de 'Consolidado EE' sabemos que 'Fecha/Hora_x' y 'Kw de Frio' no sirven\n",
    "cols_to_drop_ee = ['Fecha/Hora_x', 'Kw de Frio']\n",
    "cols_to_drop = columns_unnamed + cols_id + cols_to_drop_ee # columnas a eliminar\n",
    "\n",
    "# ahora filtramos solo las que realmente existen en el dataframe\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in df_unified_raw.columns]\n",
    "print(f'\\nColumnas a eliminar: {existing_cols_to_drop}')\n",
    "df_unified_cleaned = df_unified_raw.drop(columns=existing_cols_to_drop)\n",
    "print(f'Dataframe después de eliminar columnas redundantes: {df_unified_cleaned.shape[0]} filas, {df_unified_cleaned.shape[1]} columnas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87403da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columnas con sufijos a corregir: ['KW Gral Planta_x', 'KW CO2_x', 'Fecha/Hora_y', 'KW Gral Planta_y', 'KW CO2_y']\n",
      "Bases de columnas con sufijos: ['Fecha/Hora', 'KW CO2', 'KW Gral Planta']\n"
     ]
    }
   ],
   "source": [
    "# 2. corregimos las columnas con sufijos _x y _y generados por los merges\n",
    "\n",
    "cols_with_suffixes = [col for col in df_unified_cleaned.columns if col.endswith('_x') or col.endswith('_y')] # identificamos columnas con sufijos\n",
    "base_with_suffixes = sorted(list(set([col.replace('_x', '').replace('_y', '') for col in cols_with_suffixes]))) # identificamos las bases de esas columnas\n",
    "\n",
    "print(f'\\nColumnas con sufijos a corregir: {cols_with_suffixes}')\n",
    "print(f'Bases de columnas con sufijos: {base_with_suffixes}')\n",
    "\n",
    "cols_with_suffixes_to_drop = [] # columnas a eliminar después de la corrección\n",
    "for base in base_with_suffixes:\n",
    "    col_x = base + '_x'\n",
    "    col_y = base + '_y'\n",
    "    cols_to_compare = []\n",
    "    if col_x in df_unified_cleaned.columns:\n",
    "        cols_to_compare.append(col_x)\n",
    "    if col_y in df_unified_cleaned.columns:\n",
    "        cols_to_compare.append(col_y)\n",
    "    \n",
    "    if len(cols_to_compare) == 2:\n",
    "        # comparamos si son (casi) iguales , ignorando NaNs\n",
    "        are_equal = df_unified_cleaned[col_x].equals(df_unified_cleaned[col_y])\n",
    "        # aca nos vamos a quedar con _x (de Consolidado EE) y eliminamos _y\n",
    "        cols_to_keep = col_x\n",
    "        cols_to_rename = base\n",
    "        cols_to_drop = col_y\n",
    "        df_unified_cleaned.rename(columns={cols_to_keep: cols_to_rename}, inplace=True)\n",
    "        cols_with_suffixes_to_drop.append(cols_to_drop)\n",
    "    elif len(cols_to_compare) == 1:\n",
    "        # si solo existe una de las dos, la renombramos\n",
    "        existing_col = cols_to_compare[0]\n",
    "        cols_to_rename = base\n",
    "        df_unified_cleaned.rename(columns={existing_col: cols_to_rename}, inplace=True)\n",
    "\n",
    "# eliminamos todas las columnas _y que quedaron\n",
    "if cols_with_suffixes_to_drop:\n",
    "    df_unified_cleaned.drop(columns=cols_with_suffixes_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bfdffe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " se produjeron 8 nuevos NaNs al convertir la columna KW Trafo 8 de object a float64.\n"
     ]
    }
   ],
   "source": [
    "# 3. limpiamos las columnas de distinto tipo (object a numérico)\n",
    "cols_to_clean = 'KW Trafo 8'\n",
    "if cols_to_clean in df_unified_cleaned.columns:\n",
    "    original_dtype = df_unified_cleaned[cols_to_clean].dtype\n",
    "    nulls_before = df_unified_cleaned[cols_to_clean].isnull().sum()\n",
    "    # intentamos convertir a numérico, forzando errores a NaN\n",
    "    df_unified_cleaned[cols_to_clean] = pd.to_numeric(df_unified_cleaned[cols_to_clean], errors='coerce')\n",
    "    nulls_after = df_unified_cleaned[cols_to_clean].isnull().sum()\n",
    "    new_dtype = df_unified_cleaned[cols_to_clean].dtype\n",
    "    if nulls_after > nulls_before:\n",
    "        print(f' se produjeron {nulls_after - nulls_before} nuevos NaNs al convertir la columna {cols_to_clean} de {original_dtype} a {new_dtype}.')\n",
    "else:\n",
    "    print(f'\\n La columna {cols_to_clean} no existe en el dataframe para limpiar.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d7a6a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del DataFrame limpio: 43515 filas, 102 columnas.\n",
      "\n",
      "Primeras filas del DataFrame limpio:\n",
      "         DIA      HORA  Planta (Kw)  Elaboracion (Kw)  Bodega (Kw)  \\\n",
      "0 2022-07-01  02:00:00      1368.76              46.0        101.5   \n",
      "1 2022-07-01  03:00:00      2765.64              93.0        203.5   \n",
      "2 2022-07-01  04:00:00      4124.46             141.0        306.0   \n",
      "3 2022-07-01  05:00:01      5419.31             182.5        401.5   \n",
      "4 2022-07-01  06:00:01      6673.19             228.0        499.0   \n",
      "\n",
      "   Cocina (Kw)  Envasado (Kw)  Linea 2 (Kw)  Linea 3 (Kw)  Linea 4 (Kw)  ...  \\\n",
      "0          7.0           12.0         30.01          42.0           0.0  ...   \n",
      "1         13.0           26.0         59.64          84.0           0.0  ...   \n",
      "2         20.0           38.0         89.21         124.0           0.0  ...   \n",
      "3         26.0           50.0        117.81         164.0           0.0  ...   \n",
      "4         33.0           62.0        146.19         202.0           0.0  ...   \n",
      "\n",
      "   KW Filtr Carbon  KW Atlas 3  KW Toma Agua  KW Enfr Agua Cocina  \\\n",
      "0             15.0         0.0           0.0                  0.0   \n",
      "1             30.0         0.0           1.0                  0.0   \n",
      "2             44.0         0.0           1.0                  0.0   \n",
      "3             59.0         0.0           2.0                  0.0   \n",
      "4             73.0         0.0           2.0                  0.0   \n",
      "\n",
      "   KW Enfluentes Coc  KW Enfluente Efl  KW Enfluentes Hidr  \\\n",
      "0                4.0              17.0                 1.0   \n",
      "1                8.0              34.0                 3.0   \n",
      "2               12.0              52.0                 4.0   \n",
      "3               16.0              67.0                 5.0   \n",
      "4               19.0              85.0                 6.0   \n",
      "\n",
      "   KW Obrador Contratistas  Kw Compresores Aire  Kw Linea Barriles  \n",
      "0                      3.0                123.0                2.0  \n",
      "1                      6.0                247.0                4.0  \n",
      "2                      9.0                402.0                6.0  \n",
      "3                     12.0                558.0                9.0  \n",
      "4                     15.0                714.0               11.0  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "\n",
      "Información del DataFrame limpio:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43515 entries, 0 to 43514\n",
      "Data columns (total 102 columns):\n",
      " #    Column                    Non-Null Count  Dtype         \n",
      "---   ------                    --------------  -----         \n",
      " 0    DIA                       42900 non-null  datetime64[ns]\n",
      " 1    HORA                      42900 non-null  object        \n",
      " 2    Planta (Kw)               42902 non-null  float64       \n",
      " 3    Elaboracion (Kw)          42902 non-null  float64       \n",
      " 4    Bodega (Kw)               42902 non-null  float64       \n",
      " 5    Cocina (Kw)               42902 non-null  float64       \n",
      " 6    Envasado (Kw)             42902 non-null  float64       \n",
      " 7    Linea 2 (Kw)              42894 non-null  float64       \n",
      " 8    Linea 3 (Kw)              42902 non-null  float64       \n",
      " 9    Linea 4 (Kw)              42902 non-null  float64       \n",
      " 10   Servicios (Kw)            42902 non-null  float64       \n",
      " 11   Sala Maq (Kw)             42902 non-null  float64       \n",
      " 12   Aire (Kw)                 42902 non-null  float64       \n",
      " 13   Calderas (Kw)             42902 non-null  float64       \n",
      " 14   Efluentes (Kw)            42902 non-null  float64       \n",
      " 15   Frio (Kw)                 42902 non-null  float64       \n",
      " 16   Pta Agua / Eflu (Kw)      42902 non-null  float64       \n",
      " 17   Prod Agua (Kw)            42902 non-null  float64       \n",
      " 18   Resto Serv (Kw)           42902 non-null  float64       \n",
      " 19   Restos Planta (Kw)        43515 non-null  float64       \n",
      " 20   KW Gral Planta            42898 non-null  float64       \n",
      " 21   KW CO2                    27325 non-null  float64       \n",
      " 22   Hl de Mosto               43506 non-null  float64       \n",
      " 23   Hl Cerveza Cocina         43506 non-null  float64       \n",
      " 24   Hl Producido Bodega       43506 non-null  float64       \n",
      " 25   Hl Cerveza Filtrada       43506 non-null  float64       \n",
      " 26   Hl Cerveza Envasada       43506 non-null  float64       \n",
      " 27   Hl Cerveza L2             42891 non-null  float64       \n",
      " 28   Hl Cerveza L3             42891 non-null  float64       \n",
      " 29   Hl Cerveza L4             42891 non-null  float64       \n",
      " 30   Hl Cerveza L5             42891 non-null  float64       \n",
      " 31   Cocimientos Diarios       36469 non-null  float64       \n",
      " 32   Fecha/Hora                34383 non-null  object        \n",
      " 33   Hl de Mosto Copia         34383 non-null  float64       \n",
      " 34   Conversion Kg/Mj          43515 non-null  float64       \n",
      " 35   Gas Planta (Mj)           43515 non-null  float64       \n",
      " 36   Vapor Elaboracion (Kg)    43515 non-null  float64       \n",
      " 37   Vapor Cocina (Kg)         43515 non-null  float64       \n",
      " 38   Vapor Envasado (Kg)       43515 non-null  float64       \n",
      " 39   Vapor Servicio (Kg)       42900 non-null  float64       \n",
      " 40   ET Elaboracion (Mj)       43515 non-null  float64       \n",
      " 41   ET Envasado (Mj)          43515 non-null  float64       \n",
      " 42   ET Servicios (Mj)         43515 non-null  float64       \n",
      " 43   Tot_Vapor_L3_L4           42900 non-null  float64       \n",
      " 44   VAPOR DE LINEA 1 Y 2 KG   42899 non-null  float64       \n",
      " 45   VAPOR DE LINEA 4 KG       42900 non-null  float64       \n",
      " 46   Vapor_L5 (KG)             33774 non-null  float64       \n",
      " 47   Tot_Vapor_CIP_Bodega      42900 non-null  float64       \n",
      " 48   Vapor L3                  43515 non-null  float64       \n",
      " 49   Tot Vap Paste L3 / Hora   34387 non-null  float64       \n",
      " 50   Tot Vap Lav L3 / Hora     34387 non-null  float64       \n",
      " 51   Medicion Gas Planta (M3)  33774 non-null  float64       \n",
      " 52   Vapor _Vapor_L5 (KG)      9126 non-null   float64       \n",
      " 53   KW Trafo 4                42900 non-null  float64       \n",
      " 54   KW Trafo 5                42900 non-null  float64       \n",
      " 55   KW Trafo 8                42892 non-null  float64       \n",
      " 56   KW Trafo 11               42900 non-null  float64       \n",
      " 57   KW Trafo 12               42900 non-null  float64       \n",
      " 58   KW Linea 2                42900 non-null  float64       \n",
      " 59   KW Mycom 1                42900 non-null  float64       \n",
      " 60   KW Mycom 2                42898 non-null  float64       \n",
      " 61   KW Mycom 3                42900 non-null  float64       \n",
      " 62   KW Mycom 4                42900 non-null  float64       \n",
      " 63   KW Mycom 5                42900 non-null  float64       \n",
      " 64   KW Mycom 6                42900 non-null  float64       \n",
      " 65   KW Mycom 7                42900 non-null  float64       \n",
      " 66   KW Trafo 9                42900 non-null  float64       \n",
      " 67   KW Trafo 10               42900 non-null  float64       \n",
      " 68   KW Cocina                 42900 non-null  float64       \n",
      " 69   Kw Molino                 42900 non-null  float64       \n",
      " 70   KW Linea 3 y 4            42900 non-null  float64       \n",
      " 71   KW Linea 3                42900 non-null  float64       \n",
      " 72   KW Linea 4                42900 non-null  float64       \n",
      " 73   KW Servicio L2            42900 non-null  float64       \n",
      " 74   KW Planta de Agua         42900 non-null  float64       \n",
      " 75   Kw llum/Serv L2           42899 non-null  float64       \n",
      " 76   Kw Casona                 42899 non-null  float64       \n",
      " 77   KW Laboratorio            42899 non-null  float64       \n",
      " 78   Kw Admininistracion       42899 non-null  float64       \n",
      " 79   KW Pta Agua/Log           42899 non-null  float64       \n",
      " 80   KW Bba Glicol Sala MAq    42899 non-null  float64       \n",
      " 81   KW Comp Kaeser            42899 non-null  float64       \n",
      " 82   KW Iluminacion L3         42899 non-null  float64       \n",
      " 83   KW Iluminacion L4         42899 non-null  float64       \n",
      " 84   KW Ilum Dep L3/L4         42899 non-null  float64       \n",
      " 85   KW Cond 5. 6 y 9          42899 non-null  float64       \n",
      " 86   KW Cond 7. 8 y 11         42899 non-null  float64       \n",
      " 87   KW Cond 11. 12 y 13       42899 non-null  float64       \n",
      " 88   KW Bba Glicol Bod         42899 non-null  float64       \n",
      " 89   KW Secador Kaeser         42899 non-null  float64       \n",
      " 90   KW Caldera 3              42899 non-null  float64       \n",
      " 91   KW Caldera 4              42899 non-null  float64       \n",
      " 92   KW Filtr Carbon           42899 non-null  float64       \n",
      " 93   KW Atlas 3                42899 non-null  float64       \n",
      " 94   KW Toma Agua              42899 non-null  float64       \n",
      " 95   KW Enfr Agua Cocina       42899 non-null  float64       \n",
      " 96   KW Enfluentes Coc         42899 non-null  float64       \n",
      " 97   KW Enfluente Efl          42899 non-null  float64       \n",
      " 98   KW Enfluentes Hidr        42899 non-null  float64       \n",
      " 99   KW Obrador Contratistas   42899 non-null  float64       \n",
      " 100  Kw Compresores Aire       42900 non-null  float64       \n",
      " 101  Kw Linea Barriles         42899 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(99), object(2)\n",
      "memory usage: 33.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# ahora mostramos como quedo quedó el dataframe \"limpio\"\n",
    "print(f'Dimensiones del DataFrame limpio: {df_unified_cleaned.shape[0]} filas, {df_unified_cleaned.shape[1]} columnas.')\n",
    "print('\\nPrimeras filas del DataFrame limpio:')\n",
    "print(df_unified_cleaned.head())\n",
    "print('\\nInformación del DataFrame limpio:')\n",
    "df_unified_cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "715678e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado final del dataframe unificado: 43515 filas, 102 columnas.\n",
      "\n",
      "Primeras filas del dataframe unificado:\n",
      "         DIA      HORA  Planta (Kw)  Elaboracion (Kw)  Bodega (Kw)  \\\n",
      "0 2022-07-01  02:00:00      1368.76              46.0        101.5   \n",
      "1 2022-07-01  03:00:00      2765.64              93.0        203.5   \n",
      "2 2022-07-01  04:00:00      4124.46             141.0        306.0   \n",
      "3 2022-07-01  05:00:01      5419.31             182.5        401.5   \n",
      "4 2022-07-01  06:00:01      6673.19             228.0        499.0   \n",
      "\n",
      "   Cocina (Kw)  Envasado (Kw)  Linea 2 (Kw)  Linea 3 (Kw)  Linea 4 (Kw)  ...  \\\n",
      "0          7.0           12.0         30.01          42.0           0.0  ...   \n",
      "1         13.0           26.0         59.64          84.0           0.0  ...   \n",
      "2         20.0           38.0         89.21         124.0           0.0  ...   \n",
      "3         26.0           50.0        117.81         164.0           0.0  ...   \n",
      "4         33.0           62.0        146.19         202.0           0.0  ...   \n",
      "\n",
      "   KW Filtr Carbon  KW Atlas 3  KW Toma Agua  KW Enfr Agua Cocina  \\\n",
      "0             15.0         0.0           0.0                  0.0   \n",
      "1             30.0         0.0           1.0                  0.0   \n",
      "2             44.0         0.0           1.0                  0.0   \n",
      "3             59.0         0.0           2.0                  0.0   \n",
      "4             73.0         0.0           2.0                  0.0   \n",
      "\n",
      "   KW Enfluentes Coc  KW Enfluente Efl  KW Enfluentes Hidr  \\\n",
      "0                4.0              17.0                 1.0   \n",
      "1                8.0              34.0                 3.0   \n",
      "2               12.0              52.0                 4.0   \n",
      "3               16.0              67.0                 5.0   \n",
      "4               19.0              85.0                 6.0   \n",
      "\n",
      "   KW Obrador Contratistas  Kw Compresores Aire  Kw Linea Barriles  \n",
      "0                      3.0                123.0                2.0  \n",
      "1                      6.0                247.0                4.0  \n",
      "2                      9.0                402.0                6.0  \n",
      "3                     12.0                558.0                9.0  \n",
      "4                     15.0                714.0               11.0  \n",
      "\n",
      "[5 rows x 102 columns]\n",
      "Dataframe unificado guardado en ../data\\df_unified_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# Reasignamos el dataframe para que apunte a la versión limpia\n",
    "df_unified_raw = df_unified_cleaned\n",
    "\n",
    "if 'df_unified_raw' in locals() and not df_unified_raw.empty:\n",
    "        print(f'Resultado final del dataframe unificado: {df_unified_raw.shape[0]} filas, {df_unified_raw.shape[1]} columnas.')\n",
    "        print('\\nPrimeras filas del dataframe unificado:')\n",
    "        print(df_unified_raw.head())\n",
    "        # guardamos el dataframe unificado en un archivo CSV\n",
    "        output_file = os.path.join(data_path, 'df_unified_raw.csv')\n",
    "        try:\n",
    "            df_unified_raw.to_csv(output_file, index=False, decimal='.')\n",
    "            print(f'Dataframe unificado guardado en {output_file}')\n",
    "        except Exception as e_save:\n",
    "            print(f\"Error al guardar el dataframe unificado: {e_save}\")\n",
    "else:\n",
    "        print('\\n No se pudo crear el dataframe unificado.')\n",
    "        df_unified_raw = pd.DataFrame()  # creamos un dataframe vacío para evitar errores posteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b841c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo de checksum no encontrado, se creará uno nuevo en ../data\\checksum.json.\n",
      "Checksum guardado en ../data\\checksum.json: 112ef4a3088ea4942f0c57797aa99d9e\n"
     ]
    }
   ],
   "source": [
    "# procedemos a calcular el checksum del dataframe unificado\n",
    "\n",
    "# nos aseguramos que df_unified_raw exista y no esté vacío\n",
    "if 'df_unified_raw' in locals() and not df_unified_raw.empty:\n",
    "    # ordenamos las columnas para asegurar consistencia\n",
    "    if 'DIA' in df_unified_raw.columns and 'HORA' in df_unified_raw.columns:\n",
    "        df_unified_raw['HORA'] = df_unified_raw['HORA'].astype(str)\n",
    "        df_unified_raw = df_unified_raw.sort_values(by=['DIA', 'HORA']).reset_index(drop=True) # ordenamos por fecha y hora\n",
    "    elif 'DIA' in df_unified_raw.columns:\n",
    "        df_unified_raw = df_unified_raw.sort_values(by=['DIA']).reset_index(drop=True) # ordenamos solo por fecha si 'HORA' no existe\n",
    "    else:\n",
    "        print(\"La columna 'DIA' no existe en el dataframe, no se puede ordenar.\")\n",
    "    \n",
    "    # convertimos a bytes para hashear\n",
    "    data_bytes = df_unified_raw.to_csv(index=False, decimal='.').encode()\n",
    "    checksum = hashlib.md5(data_bytes).hexdigest()\n",
    "    \n",
    "    # lo guardamos\n",
    "    checksum_file_path = os.path.join(data_path, 'checksum.json')\n",
    "    checksum_data = {}\n",
    "    try:\n",
    "        with open(checksum_file_path, 'r') as f:\n",
    "            checksum_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Archivo de checksum no encontrado, se creará uno nuevo en {checksum_file_path}.')\n",
    "    except json.JSONDecodeError:\n",
    "        print(f'Error al decodificar el archivo de checksum, se creará uno nuevo en {checksum_file_path}.')\n",
    "    \n",
    "    checksum_data['df_unified_raw'] = checksum # actualizamos o agregamos el checksum\n",
    "    \n",
    "    # guardamos el archivo\n",
    "    try:\n",
    "        with open(checksum_file_path, 'w') as f:\n",
    "            json.dump(checksum_data, f, indent=4)\n",
    "        print(f'Checksum guardado en {checksum_file_path}: {checksum}')\n",
    "    except Exception as e_checksum:\n",
    "        print(f\"Error al guardar el checksum: {e_checksum}\")\n",
    "else:\n",
    "    print('\\n No se pudo calcular el checksum del dataframe unificado porque no existe o está vacío.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66f8c4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Se procesarán 42900 filas con timestamps válidos (excluyendo 615 inválidas).\n",
      "INFO: Se encontraron 1199 días únicos con registros válidos.\n",
      "Dataframe diario guardado en ../data\\df_daily.csv: 1199 filas, 100 columnas.\n"
     ]
    }
   ],
   "source": [
    "# unificación del dataframe con el ultimo registro de cada día\n",
    "\n",
    "if 'df_unified_raw' in locals() and not df_unified_raw.empty:\n",
    "    # primero verificamos nuevamente que 'DIA' sea del tipo datetime (a modo de seguro)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_unified_raw['DIA']):\n",
    "        df_unified_raw['DIA'] = pd.to_datetime(df_unified_raw['DIA'], errors='coerce')\n",
    "        \n",
    "    # creamos columnas temporales para la agregación\n",
    "    df_unified_raw['Fecha_DT_Temp'] = df_unified_raw['DIA'].dt.date\n",
    "    \n",
    "    # convertimos 'HORA' a timedelta para facilitar la comparación\n",
    "    df_unified_raw['HORA_TD_Temp'] = pd.to_timedelta(df_unified_raw['HORA'].astype(str), errors='coerce')\n",
    "    \n",
    "    # combinamos 'DIA' y 'HORA' en una sola columna datetime para identificar el último registro del día\n",
    "    df_unified_raw['Timestamp_Completo_Temp'] = pd.to_datetime(df_unified_raw['Fecha_DT_Temp']) + df_unified_raw['HORA_TD_Temp']\n",
    "    \n",
    "    # lógica para obtener el último registro de cada día\n",
    "    \n",
    "    # 1. filtramos las filas donde Timestamp_Completo_Temp es inválido O Fecha_DT_Temp es NaT\n",
    "    df_valid = df_unified_raw.dropna(subset=['Fecha_DT_Temp', 'Timestamp_Completo_Temp']).copy()\n",
    "    if df_valid.shape[0] < df_unified_raw.shape[0]:\n",
    "        print(f\"INFO: Se procesarán {df_valid.shape[0]} filas con timestamps válidos (excluyendo {df_unified_raw.shape[0] - df_valid.shape[0]} inválidas).\")\n",
    "    \n",
    "    # 2. obtenemos el índice del último registro por día\n",
    "    idx_last_hours = df_valid.groupby('Fecha_DT_Temp')['Timestamp_Completo_Temp'].idxmax()\n",
    "    print(f\"INFO: Se encontraron {len(idx_last_hours)} días únicos con registros válidos.\")\n",
    "    \n",
    "    # 3. usamos los índices para seleccionar las filas correspondientes\n",
    "    df_daily = df_valid.loc[idx_last_hours].reset_index(drop=True)\n",
    "    \n",
    "    # creamos la columna unicamente con la fecha (sin hora)\n",
    "    df_daily['Fecha'] = pd.to_datetime(df_daily['Fecha_DT_Temp'])\n",
    "    df_daily.set_index('Fecha', inplace=True)\n",
    "    \n",
    "    # eliminamos las columnas temporales\n",
    "    df_daily.drop(columns=['DIA','HORA','Fecha_DT_Temp', 'HORA_TD_Temp', 'Timestamp_Completo_Temp'], inplace=True)\n",
    "    \n",
    "    # guardamos el dataframe diario\n",
    "    output_daily_file = os.path.join(data_path, 'df_daily.csv')\n",
    "    try:\n",
    "        df_daily.to_csv(output_daily_file, index=True, decimal='.')\n",
    "        print(f'Dataframe diario guardado en {output_daily_file}: {df_daily.shape[0]} filas, {df_daily.shape[1]} columnas.')\n",
    "    except Exception as e_save_daily:\n",
    "        print(f\"Error al guardar el dataframe diario: {e_save_daily}\")\n",
    "else:\n",
    "    print('\\n No se pudo crear el dataframe diario porque el dataframe unificado no existe o está vacío.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72e17551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum del dataframe diario guardado en ../data\\checksum.json: 59e86be030e880a8b735e2f2c5a4e6fd\n"
     ]
    }
   ],
   "source": [
    "# hacemos un nuevo checksum para el dataframe diario\n",
    "if 'df_daily' in locals() and not df_daily.empty:\n",
    "    # ordenamos por índice (Fecha) para consistencia\n",
    "    df_daily = df_daily.sort_index().reset_index()\n",
    "    \n",
    "    # convertimos a bytes para hashear\n",
    "    data_daily_bytes = df_daily.to_csv(index=False, decimal='.').encode()\n",
    "    checksum_daily = hashlib.md5(data_daily_bytes).hexdigest()\n",
    "    \n",
    "    # lo guardamos\n",
    "    checksum_file_path = os.path.join(data_path, 'checksum.json')\n",
    "    checksum_data = {}\n",
    "    try:\n",
    "        with open(checksum_file_path, 'r') as f:\n",
    "            checksum_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f'Archivo de checksum no encontrado, se creará uno nuevo en {checksum_file_path}.')\n",
    "    except json.JSONDecodeError:\n",
    "        print(f'Error al decodificar el archivo de checksum, se creará uno nuevo en {checksum_file_path}.')\n",
    "    \n",
    "    checksum_data['df_daily'] = checksum_daily # actualizamos o agregamos el checksum\n",
    "    \n",
    "    # guardamos el archivo\n",
    "    try:\n",
    "        with open(checksum_file_path, 'w') as f:\n",
    "            json.dump(checksum_data, f, indent=4)\n",
    "        print(f'Checksum del dataframe diario guardado en {checksum_file_path}: {checksum_daily}')\n",
    "    except Exception as e_checksum_daily:\n",
    "        print(f\"Error al guardar el checksum del dataframe diario: {e_checksum_daily}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cervecera_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
